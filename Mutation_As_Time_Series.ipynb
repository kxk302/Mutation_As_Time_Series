{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mutation_As_Time_Series",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPH7MxWxx4AxGfxU7wbVWfA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kxk302/Mutation_As_Time_Series/blob/main/Mutation_As_Time_Series.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwCiqdt8a0bt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eedebce-e111-4f4d-c5cb-35dd5414fe18"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu33b9McbRmn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbce6070-e69d-4adb-a966-9a08639547d9"
      },
      "source": [
        "!ls '/content/gdrive/MyDrive/Colab Notebooks/data'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_pivoted.tsv  data.tsv   testY.tsv\t trainY.tsv\n",
            "data_sorted.tsv   testX.tsv  trainX.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1pO3FlQbWvr"
      },
      "source": [
        "#\n",
        "# Sort the dataset based on Collection_Date and Sample, in ascending order\n",
        "#\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from dateutil import parser\n",
        "\n",
        "pd.set_option('max_rows', None)\n",
        "\n",
        "df_in = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/data/data.tsv', sep='\\t', names=['Sample', 'Collection_Date', 'UNK1', 'UNK2', 'UNK3', 'POS', 'REF', 'ALT', 'EFFECT', 'CODON', 'TRID', 'AA', 'AF'])\n",
        "\n",
        "# DEBUG\n",
        "# df_in = df_in.iloc[:1000,:]\n",
        "\n",
        "print(type(df_in.Collection_Date[0]))\n",
        "df_in.Collection_Date = df_in.Collection_Date.apply(lambda x: parser.parse(x))\n",
        "print(type(df_in.Collection_Date[0]))\n",
        "\n",
        "df_in.sort_values(by=['Collection_Date', 'Sample'], ascending=[True, True], inplace=True)\n",
        "print('\\n\\n')\n",
        "print(df_in.shape)\n",
        "print(df_in.head(1000))\n",
        "\n",
        "# df_in.to_csv('/content/gdrive/MyDrive/Colab Notebooks/data/data_sorted.tsv', sep='\\t', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EZdu7nDm9BE"
      },
      "source": [
        "#\n",
        "# Filter the dataset, calculate normalized mutation counts, pivot the data, and save to file\n",
        "#\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from dateutil import parser\n",
        "\n",
        "pd.set_option('max_rows', None)\n",
        "pd.set_option('max_columns', None)\n",
        "\n",
        "start_date = '2021-01-27'\n",
        "end_date = '2021-08-09'\n",
        "file_name = '/content/gdrive/MyDrive/Colab Notebooks/data/data_sorted.tsv'\n",
        "separator = '\\t'\n",
        "# NON_SYNONYMOUS_CODING: missense mutation\n",
        "effects = ['NON_SYNONYMOUS_CODING']\n",
        "drop_columns = ['UNK1', 'UNK2', 'UNK3']\n",
        "\n",
        "# Index of nucleotide changes\n",
        "#\n",
        "# 0: A->T\n",
        "# 1: A->G\n",
        "# 2: A->C\n",
        "#\n",
        "# 3: T->G\n",
        "# 4: T->C\n",
        "# 5: T->A\n",
        "#\n",
        "# 6: G->C\n",
        "# 7: G->A\n",
        "# 8: G->T\n",
        "#\n",
        "# 9:  C->A\n",
        "# 10: C->T\n",
        "# 11: C->G\n",
        "#\n",
        "atgc_dict = {'A': {'T': 0, 'G': 1, 'C': 2}, 'T': {'G': 3, 'C': 4, 'A': 5}, 'G': {'C': 6, 'A': 7, 'T': 8}, 'C': {'A': 9, 'T':10, 'G': 11}}\n",
        "\n",
        "# Read the input file\n",
        "df_in = pd.read_csv(file_name, sep=separator)\n",
        "\n",
        "# Drop the unnecessary columns\n",
        "df_in.drop(columns=drop_columns, inplace=True)\n",
        "\n",
        "# Select only rows with mutation type specified in 'effects' list\n",
        "df_eff = df_in[ df_in.EFFECT.isin(effects) ]\n",
        "\n",
        "# Select only rows where the Collection_Date falls between start_date and end_date \n",
        "df_fil = df_eff[ (df_eff.Collection_Date >= start_date) & (df_eff.Collection_Date <= end_date) ]\n",
        "print('\\n\\n')\n",
        "print('Filtered df shape {}'.format(df_fil.shape))\n",
        "print('Number of unique dates {}'.format(len(df_fil.Collection_Date.unique())))\n",
        " \n",
        "df_fil['nucleotide_change'] = df_fil.apply(lambda x: atgc_dict[x.REF][x.ALT], axis=1)\n",
        "print('Calculating normalize_nucleotide_change')\n",
        "normalize_nucleotide_change = df_fil.groupby(df_fil.Collection_Date).nucleotide_change.value_counts() / df_fil.groupby(df_fil.Collection_Date).nucleotide_change.count()\n",
        "print('\\n\\n')\n",
        "print('Type of normalize_nucleotide_change')\n",
        "print(type(normalize_nucleotide_change))\n",
        "print('Name of normalize_nucleotide_change BEFORE rename')\n",
        "print(normalize_nucleotide_change.name)\n",
        "normalize_nucleotide_change = normalize_nucleotide_change.rename( 'normalize_nucleotide_change')\n",
        "print('Name of normalize_nucleotide_change AFTER rename')\n",
        "print(normalize_nucleotide_change.name)\n",
        "print('Index of normalize_nucleotide_change')\n",
        "print(normalize_nucleotide_change.index)\n",
        "\n",
        "# Convert normalize_nucleotide_change Series to Dataframe\n",
        "df = normalize_nucleotide_change.to_frame()\n",
        "\n",
        "print('\\n\\n')\n",
        "print('Type of df')\n",
        "print(type(df))\n",
        "print('Columns of df')\n",
        "print(df.columns)\n",
        "print('df.head(5)')\n",
        "print(df.head(5))\n",
        "print('df.index BEFORE reset')\n",
        "print(df.index)\n",
        "df.reset_index(inplace=True)\n",
        "print('df.index AFTER reset')\n",
        "print(df.index)\n",
        "print('df.head(5) AFTER reset')\n",
        "print(df.head(25))\n",
        "\n",
        "df_piv = pd.pivot_table(df, index='Collection_Date', columns='nucleotide_change', values='normalize_nucleotide_change')\n",
        "df_piv.fillna(0, inplace=True)\n",
        "print('\\n\\n')\n",
        "print('Pivoted df')\n",
        "print(df_piv.head(5))\n",
        "print('Index of df_piv')\n",
        "print(df_piv.index)\n",
        "print('Columns of df_piv')\n",
        "print(df_piv.columns)\n",
        "print('Shape of df_piv')\n",
        "print(df_piv.shape)\n",
        "\n",
        "df_piv.to_csv('/content/gdrive/MyDrive/Colab Notebooks/data/data_pivoted.tsv', sep=separator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "9yYlq1g82id2",
        "outputId": "8300231f-3cd9-44e5-cdce-b305866511ea"
      },
      "source": [
        "#\n",
        "# Create time series prediction dataset from pivoted data\n",
        "#\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from dateutil import parser\n",
        "\n",
        "pd.set_option('max_rows', None)\n",
        "pd.set_option('max_columns', None)\n",
        "\n",
        "file_name = '/content/gdrive/MyDrive/Colab Notebooks/data/data_pivoted.tsv'\n",
        "separator = '\\t'\n",
        "train_data_percentage = 0.80\n",
        "\n",
        "# Read the input file\n",
        "df_in = pd.read_csv(file_name, sep=separator)\n",
        "\n",
        "# Drop the Collection_Date column\n",
        "df_in.drop(columns='Collection_Date', inplace=True)\n",
        "\n",
        "print(df_in.head(5))\n",
        "print(df_in.shape[0])\n",
        "print(df_in.shape[1])\n",
        "\n",
        "trainX = []\n",
        "trainY = []\n",
        "\n",
        "num_future = 1\n",
        "num_past = 14\n",
        "num_rows = df_in.shape[0]\n",
        "num_cols = df_in.shape[1]\n",
        "\n",
        "for i in range(num_past, num_rows - num_future + 1):\n",
        "  trainX.append(df_in.iloc[i - num_past:i, 0:num_cols])\n",
        "  trainY.append(df_in.iloc[i + num_future - 1:i + num_future, 0:num_cols])\n",
        "\n",
        "trainX, trainY = np.array(trainX), np.array(trainY)\n",
        "\n",
        "print('trainX shape == {}'.format(trainX.shape))\n",
        "print('trainY shape == {}'.format(trainY.shape))\n",
        "\n",
        "# trainX = trainX.reshape(trainX.shape[0], trainX.shape[1] * trainX.shape[2])\n",
        "trainY = trainY.reshape(trainY.shape[0], trainY.shape[1] * trainY.shape[2])\n",
        "\n",
        "# print('trainX shape == {}'.format(trainX.shape))\n",
        "print('trainY shape == {}'.format(trainY.shape))\n",
        "\n",
        "train_data_idx = int(trainX.shape[0] * train_data_percentage)\n",
        "\n",
        "testX = trainX[train_data_idx:,:,:]\n",
        "testY = trainY[train_data_idx:,:]\n",
        "trainX = trainX[0:train_data_idx,:,:]\n",
        "trainY = trainY[0:train_data_idx,:]\n",
        "\n",
        "print('trainX shape == {}'.format(trainX.shape))\n",
        "print('trainY shape == {}'.format(trainY.shape))\n",
        "print('testX shape == {}'.format(testX.shape))\n",
        "print('testY shape == {}'.format(testY.shape))\n",
        "\n",
        "'''\n",
        "np.savetxt('/content/gdrive/MyDrive/Colab Notebooks/data/trainX.tsv', trainX, delimiter=separator)\n",
        "np.savetxt('/content/gdrive/MyDrive/Colab Notebooks/data/trainY.tsv', trainY, delimiter=separator)\n",
        "np.savetxt('/content/gdrive/MyDrive/Colab Notebooks/data/testX.tsv', testX, delimiter=separator)\n",
        "np.savetxt('/content/gdrive/MyDrive/Colab Notebooks/data/testY.tsv', testY, delimiter=separator)\n",
        "'''"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          0         1         2         3         4         5         6  \\\n",
            "0  0.088870  0.123095  0.001726  0.058959  0.064423  0.043716  0.131435   \n",
            "1  0.084602  0.116751  0.003384  0.059222  0.062606  0.042301  0.130288   \n",
            "2  0.102041  0.122449  0.000000  0.057143  0.069388  0.048980  0.134694   \n",
            "3  0.097902  0.139860  0.000000  0.076923  0.052448  0.041958  0.136364   \n",
            "4  0.090909  0.102273  0.000000  0.056818  0.045455  0.045455  0.136364   \n",
            "\n",
            "          7         8         9        10        11  \n",
            "0  0.053494  0.067875  0.127984  0.237274  0.001150  \n",
            "1  0.059222  0.069374  0.121827  0.250423  0.000000  \n",
            "2  0.053061  0.053061  0.142857  0.216327  0.000000  \n",
            "3  0.052448  0.066434  0.122378  0.209790  0.003497  \n",
            "4  0.056818  0.056818  0.136364  0.272727  0.000000  \n",
            "191\n",
            "12\n",
            "trainX shape == (177, 14, 12)\n",
            "trainY shape == (177, 1, 12)\n",
            "trainY shape == (177, 12)\n",
            "trainX shape == (141, 14, 12)\n",
            "trainY shape == (141, 12)\n",
            "testX shape == (36, 14, 12)\n",
            "testY shape == (36, 12)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nnp.savetxt('/content/gdrive/MyDrive/Colab Notebooks/data/trainX.tsv', trainX, delimiter=separator)\\nnp.savetxt('/content/gdrive/MyDrive/Colab Notebooks/data/trainY.tsv', trainY, delimiter=separator)\\nnp.savetxt('/content/gdrive/MyDrive/Colab Notebooks/data/testX.tsv', testX, delimiter=separator)\\nnp.savetxt('/content/gdrive/MyDrive/Colab Notebooks/data/testY.tsv', testY, delimiter=separator)\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c7WB0W3GBH8",
        "outputId": "98a63bb8-e5d5-4ff4-d26d-349f7730f3a0"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, LSTM\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "'''\n",
        "model = Sequential()\n",
        "model.add(LSTM(500, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences=True))\n",
        "model.add(Dense(250, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(150, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(trainY.shape[1]))\n",
        "'''\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(500, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences=True))\n",
        "model.add(Dense(250, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(150, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(trainY.shape[1]))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(trainX, trainY, epochs=250, batch_size=15, validation_split=0.2, verbose=1)\n",
        "print('Training loss')\n",
        "print(history.history)\n",
        "\n",
        "predicted_output = model.predict(testX, verbose=0)\n",
        "predicted_mse = np.sqrt(mean_squared_error(testY, predicted_output))\n",
        "print('MSE of Test dataset')\n",
        "print(predicted_mse)\n",
        "\n",
        "# Make a single prediction\n",
        "testX_0 = testX[0].reshape(1, testX.shape[1], testX.shape[2])\n",
        "testY_0 = testY[0].reshape(1, testY.shape[1])\n",
        "\n",
        "print('Predicted output')\n",
        "predicted_output_0 = model.predict(testX_0, verbose=0)\n",
        "print(predicted_output_0)\n",
        "print('Actual output')\n",
        "print(testY_0)\n",
        "print('MSE of prediction')\n",
        "np.sqrt(mean_squared_error(testY_0, predicted_output_0))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_18 (LSTM)              (None, 14, 500)           1026000   \n",
            "                                                                 \n",
            " dense_54 (Dense)            (None, 14, 250)           125250    \n",
            "                                                                 \n",
            " dropout_36 (Dropout)        (None, 14, 250)           0         \n",
            "                                                                 \n",
            " dense_55 (Dense)            (None, 14, 150)           37650     \n",
            "                                                                 \n",
            " dropout_37 (Dropout)        (None, 14, 150)           0         \n",
            "                                                                 \n",
            " dense_56 (Dense)            (None, 14, 50)            7550      \n",
            "                                                                 \n",
            " flatten_18 (Flatten)        (None, 700)               0         \n",
            "                                                                 \n",
            " dense_57 (Dense)            (None, 12)                8412      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,204,862\n",
            "Trainable params: 1,204,862\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/250\n",
            "8/8 [==============================] - 2s 100ms/step - loss: 0.0050 - val_loss: 0.0047\n",
            "Epoch 2/250\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 0.0011 - val_loss: 0.0030\n",
            "Epoch 3/250\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 6.8829e-04 - val_loss: 0.0040\n",
            "Epoch 4/250\n",
            "8/8 [==============================] - 1s 64ms/step - loss: 5.7059e-04 - val_loss: 0.0039\n",
            "Epoch 5/250\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 5.5905e-04 - val_loss: 0.0032\n",
            "Epoch 6/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 5.0210e-04 - val_loss: 0.0027\n",
            "Epoch 7/250\n",
            "8/8 [==============================] - 1s 75ms/step - loss: 4.9763e-04 - val_loss: 0.0026\n",
            "Epoch 8/250\n",
            "8/8 [==============================] - 1s 62ms/step - loss: 3.9275e-04 - val_loss: 0.0015\n",
            "Epoch 9/250\n",
            "8/8 [==============================] - 0s 63ms/step - loss: 2.9818e-04 - val_loss: 7.6235e-04\n",
            "Epoch 10/250\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 1.8115e-04 - val_loss: 4.2386e-04\n",
            "Epoch 11/250\n",
            "8/8 [==============================] - 0s 61ms/step - loss: 1.2236e-04 - val_loss: 3.2725e-04\n",
            "Epoch 12/250\n",
            "8/8 [==============================] - 0s 64ms/step - loss: 1.0098e-04 - val_loss: 8.6733e-05\n",
            "Epoch 13/250\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 8.1526e-05 - val_loss: 1.6780e-04\n",
            "Epoch 14/250\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 7.2474e-05 - val_loss: 3.4540e-04\n",
            "Epoch 15/250\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 7.3086e-05 - val_loss: 1.1511e-04\n",
            "Epoch 16/250\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 6.7947e-05 - val_loss: 7.2407e-05\n",
            "Epoch 17/250\n",
            "8/8 [==============================] - 1s 72ms/step - loss: 6.5408e-05 - val_loss: 1.0430e-04\n",
            "Epoch 18/250\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 6.1850e-05 - val_loss: 6.0109e-05\n",
            "Epoch 19/250\n",
            "8/8 [==============================] - 0s 60ms/step - loss: 6.6468e-05 - val_loss: 2.9686e-05\n",
            "Epoch 20/250\n",
            "8/8 [==============================] - 1s 64ms/step - loss: 8.0009e-05 - val_loss: 4.5253e-05\n",
            "Epoch 21/250\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 6.4259e-05 - val_loss: 8.1873e-05\n",
            "Epoch 22/250\n",
            "8/8 [==============================] - 0s 62ms/step - loss: 7.1438e-05 - val_loss: 2.4020e-05\n",
            "Epoch 23/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 6.7882e-05 - val_loss: 6.6413e-05\n",
            "Epoch 24/250\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 5.7493e-05 - val_loss: 7.5512e-05\n",
            "Epoch 25/250\n",
            "8/8 [==============================] - 0s 61ms/step - loss: 5.6981e-05 - val_loss: 1.0309e-04\n",
            "Epoch 26/250\n",
            "8/8 [==============================] - 0s 62ms/step - loss: 5.7558e-05 - val_loss: 1.6483e-04\n",
            "Epoch 27/250\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 5.5846e-05 - val_loss: 4.3999e-05\n",
            "Epoch 28/250\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 5.1416e-05 - val_loss: 4.3723e-05\n",
            "Epoch 29/250\n",
            "8/8 [==============================] - 1s 62ms/step - loss: 4.8218e-05 - val_loss: 1.0131e-04\n",
            "Epoch 30/250\n",
            "8/8 [==============================] - 1s 63ms/step - loss: 4.8088e-05 - val_loss: 1.3425e-04\n",
            "Epoch 31/250\n",
            "8/8 [==============================] - 0s 63ms/step - loss: 5.7167e-05 - val_loss: 7.5832e-05\n",
            "Epoch 32/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 5.6420e-05 - val_loss: 8.7956e-05\n",
            "Epoch 33/250\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 4.9633e-05 - val_loss: 6.6811e-05\n",
            "Epoch 34/250\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 4.5935e-05 - val_loss: 1.1806e-04\n",
            "Epoch 35/250\n",
            "8/8 [==============================] - 1s 78ms/step - loss: 4.9024e-05 - val_loss: 5.1559e-05\n",
            "Epoch 36/250\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 4.1571e-05 - val_loss: 5.8072e-05\n",
            "Epoch 37/250\n",
            "8/8 [==============================] - 1s 64ms/step - loss: 4.4405e-05 - val_loss: 3.4090e-05\n",
            "Epoch 38/250\n",
            "8/8 [==============================] - 1s 63ms/step - loss: 3.7099e-05 - val_loss: 4.2718e-05\n",
            "Epoch 39/250\n",
            "8/8 [==============================] - 0s 62ms/step - loss: 3.6117e-05 - val_loss: 6.3463e-05\n",
            "Epoch 40/250\n",
            "8/8 [==============================] - 1s 62ms/step - loss: 3.7284e-05 - val_loss: 4.1052e-05\n",
            "Epoch 41/250\n",
            "8/8 [==============================] - 1s 76ms/step - loss: 4.0904e-05 - val_loss: 3.7499e-05\n",
            "Epoch 42/250\n",
            "8/8 [==============================] - 1s 64ms/step - loss: 3.7879e-05 - val_loss: 5.2019e-05\n",
            "Epoch 43/250\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 3.8680e-05 - val_loss: 3.3761e-05\n",
            "Epoch 44/250\n",
            "8/8 [==============================] - 0s 62ms/step - loss: 6.7424e-05 - val_loss: 9.0969e-05\n",
            "Epoch 45/250\n",
            "8/8 [==============================] - 0s 63ms/step - loss: 5.4055e-05 - val_loss: 2.0057e-04\n",
            "Epoch 46/250\n",
            "8/8 [==============================] - 1s 64ms/step - loss: 4.6302e-05 - val_loss: 2.6179e-05\n",
            "Epoch 47/250\n",
            "8/8 [==============================] - 1s 64ms/step - loss: 4.6043e-05 - val_loss: 8.5452e-05\n",
            "Epoch 48/250\n",
            "8/8 [==============================] - 1s 64ms/step - loss: 3.7704e-05 - val_loss: 4.6482e-05\n",
            "Epoch 49/250\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 4.1242e-05 - val_loss: 2.8977e-05\n",
            "Epoch 50/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 3.8000e-05 - val_loss: 3.1343e-05\n",
            "Epoch 51/250\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 3.9055e-05 - val_loss: 4.6752e-05\n",
            "Epoch 52/250\n",
            "8/8 [==============================] - 1s 63ms/step - loss: 4.0735e-05 - val_loss: 7.9581e-05\n",
            "Epoch 53/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 3.7519e-05 - val_loss: 2.2622e-05\n",
            "Epoch 54/250\n",
            "8/8 [==============================] - 1s 63ms/step - loss: 3.7373e-05 - val_loss: 4.0638e-05\n",
            "Epoch 55/250\n",
            "8/8 [==============================] - 0s 61ms/step - loss: 5.5698e-05 - val_loss: 7.9159e-05\n",
            "Epoch 56/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 4.0415e-05 - val_loss: 3.1613e-05\n",
            "Epoch 57/250\n",
            "8/8 [==============================] - 0s 61ms/step - loss: 3.6689e-05 - val_loss: 2.7685e-05\n",
            "Epoch 58/250\n",
            "8/8 [==============================] - 0s 61ms/step - loss: 3.6140e-05 - val_loss: 5.5222e-05\n",
            "Epoch 59/250\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 4.3309e-05 - val_loss: 4.3772e-05\n",
            "Epoch 60/250\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 3.6391e-05 - val_loss: 4.8057e-05\n",
            "Epoch 61/250\n",
            "8/8 [==============================] - 1s 79ms/step - loss: 3.4814e-05 - val_loss: 6.6682e-05\n",
            "Epoch 62/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 3.7753e-05 - val_loss: 2.3050e-05\n",
            "Epoch 63/250\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 3.3231e-05 - val_loss: 8.1102e-05\n",
            "Epoch 64/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 2.7312e-05 - val_loss: 2.7783e-05\n",
            "Epoch 65/250\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 3.2218e-05 - val_loss: 5.6751e-05\n",
            "Epoch 66/250\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 3.1557e-05 - val_loss: 2.5767e-05\n",
            "Epoch 67/250\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 3.6717e-05 - val_loss: 2.7393e-05\n",
            "Epoch 68/250\n",
            "8/8 [==============================] - 1s 63ms/step - loss: 3.4984e-05 - val_loss: 2.4758e-05\n",
            "Epoch 69/250\n",
            "8/8 [==============================] - 1s 64ms/step - loss: 3.7971e-05 - val_loss: 3.2565e-05\n",
            "Epoch 70/250\n",
            "8/8 [==============================] - 1s 64ms/step - loss: 3.2836e-05 - val_loss: 4.2050e-05\n",
            "Epoch 71/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 3.2730e-05 - val_loss: 6.9990e-05\n",
            "Epoch 72/250\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 4.6494e-05 - val_loss: 6.4853e-05\n",
            "Epoch 73/250\n",
            "8/8 [==============================] - 1s 63ms/step - loss: 3.5704e-05 - val_loss: 1.7378e-04\n",
            "Epoch 74/250\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 4.4615e-05 - val_loss: 2.4414e-05\n",
            "Epoch 75/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 3.4163e-05 - val_loss: 3.0031e-05\n",
            "Epoch 76/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 3.3049e-05 - val_loss: 5.7951e-05\n",
            "Epoch 77/250\n",
            "8/8 [==============================] - 1s 63ms/step - loss: 3.1900e-05 - val_loss: 8.0549e-05\n",
            "Epoch 78/250\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 3.1914e-05 - val_loss: 2.6357e-05\n",
            "Epoch 79/250\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 3.5227e-05 - val_loss: 4.1987e-05\n",
            "Epoch 80/250\n",
            "8/8 [==============================] - 1s 76ms/step - loss: 2.9956e-05 - val_loss: 3.3806e-05\n",
            "Epoch 81/250\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 4.6592e-05 - val_loss: 2.1569e-04\n",
            "Epoch 82/250\n",
            "8/8 [==============================] - 1s 63ms/step - loss: 4.0005e-05 - val_loss: 2.9123e-05\n",
            "Epoch 83/250\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 3.0751e-05 - val_loss: 3.1373e-05\n",
            "Epoch 84/250\n",
            "8/8 [==============================] - 1s 64ms/step - loss: 3.7408e-05 - val_loss: 8.0774e-05\n",
            "Epoch 85/250\n",
            "8/8 [==============================] - 1s 63ms/step - loss: 3.5509e-05 - val_loss: 4.8879e-05\n",
            "Epoch 86/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 2.8976e-05 - val_loss: 3.0981e-05\n",
            "Epoch 87/250\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 2.8952e-05 - val_loss: 8.1787e-05\n",
            "Epoch 88/250\n",
            "8/8 [==============================] - 0s 61ms/step - loss: 4.1239e-05 - val_loss: 8.0712e-05\n",
            "Epoch 89/250\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 3.0968e-05 - val_loss: 2.7258e-05\n",
            "Epoch 90/250\n",
            "8/8 [==============================] - 1s 62ms/step - loss: 3.4446e-05 - val_loss: 5.1813e-05\n",
            "Epoch 91/250\n",
            "8/8 [==============================] - 1s 73ms/step - loss: 4.2550e-05 - val_loss: 2.1708e-05\n",
            "Epoch 92/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 3.4302e-05 - val_loss: 3.2008e-05\n",
            "Epoch 93/250\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 3.5176e-05 - val_loss: 2.0176e-05\n",
            "Epoch 94/250\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 3.5105e-05 - val_loss: 3.0486e-05\n",
            "Epoch 95/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 3.4348e-05 - val_loss: 5.2465e-05\n",
            "Epoch 96/250\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 3.7084e-05 - val_loss: 5.6972e-05\n",
            "Epoch 97/250\n",
            "8/8 [==============================] - 1s 64ms/step - loss: 3.3734e-05 - val_loss: 2.1281e-05\n",
            "Epoch 98/250\n",
            "8/8 [==============================] - 0s 61ms/step - loss: 3.0714e-05 - val_loss: 2.4151e-05\n",
            "Epoch 99/250\n",
            "8/8 [==============================] - 0s 63ms/step - loss: 2.5876e-05 - val_loss: 2.2015e-05\n",
            "Epoch 100/250\n",
            "8/8 [==============================] - 1s 64ms/step - loss: 2.5991e-05 - val_loss: 3.8958e-05\n",
            "Epoch 101/250\n",
            "8/8 [==============================] - 1s 64ms/step - loss: 3.0650e-05 - val_loss: 7.5812e-05\n",
            "Epoch 102/250\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 3.4121e-05 - val_loss: 3.0133e-05\n",
            "Epoch 103/250\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 3.0500e-05 - val_loss: 3.1056e-05\n",
            "Epoch 104/250\n",
            "8/8 [==============================] - 1s 62ms/step - loss: 2.6011e-05 - val_loss: 3.4727e-05\n",
            "Epoch 105/250\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 2.9567e-05 - val_loss: 1.7344e-05\n",
            "Epoch 106/250\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 3.3800e-05 - val_loss: 2.7874e-05\n",
            "Epoch 107/250\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 3.1656e-05 - val_loss: 5.6784e-05\n",
            "Epoch 108/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 4.5670e-05 - val_loss: 2.2079e-05\n",
            "Epoch 109/250\n",
            "8/8 [==============================] - 1s 63ms/step - loss: 3.0216e-05 - val_loss: 2.2778e-05\n",
            "Epoch 110/250\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 3.2577e-05 - val_loss: 5.1887e-05\n",
            "Epoch 111/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 3.3092e-05 - val_loss: 1.7411e-05\n",
            "Epoch 112/250\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 2.4082e-05 - val_loss: 2.2295e-05\n",
            "Epoch 113/250\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 2.6003e-05 - val_loss: 2.1372e-05\n",
            "Epoch 114/250\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 2.3949e-05 - val_loss: 1.9586e-05\n",
            "Epoch 115/250\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 2.3941e-05 - val_loss: 2.4733e-05\n",
            "Epoch 116/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 3.3164e-05 - val_loss: 4.9561e-05\n",
            "Epoch 117/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 2.7587e-05 - val_loss: 1.7548e-05\n",
            "Epoch 118/250\n",
            "8/8 [==============================] - 1s 77ms/step - loss: 3.6438e-05 - val_loss: 9.8550e-05\n",
            "Epoch 119/250\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 3.1012e-05 - val_loss: 6.0101e-05\n",
            "Epoch 120/250\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 3.3974e-05 - val_loss: 2.6652e-04\n",
            "Epoch 121/250\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 3.8221e-05 - val_loss: 4.6476e-05\n",
            "Epoch 122/250\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 3.4690e-05 - val_loss: 4.0031e-05\n",
            "Epoch 123/250\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 2.8454e-05 - val_loss: 4.4934e-05\n",
            "Epoch 124/250\n",
            "8/8 [==============================] - 1s 73ms/step - loss: 4.3154e-05 - val_loss: 5.2567e-05\n",
            "Epoch 125/250\n",
            "8/8 [==============================] - 0s 62ms/step - loss: 3.2302e-05 - val_loss: 2.2149e-04\n",
            "Epoch 126/250\n",
            "8/8 [==============================] - 1s 64ms/step - loss: 4.4899e-05 - val_loss: 3.0052e-05\n",
            "Epoch 127/250\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 3.8299e-05 - val_loss: 5.0073e-05\n",
            "Epoch 128/250\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 3.3947e-05 - val_loss: 1.7251e-05\n",
            "Epoch 129/250\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 2.6050e-05 - val_loss: 1.9075e-05\n",
            "Epoch 130/250\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 2.4812e-05 - val_loss: 2.7684e-05\n",
            "Epoch 131/250\n",
            "8/8 [==============================] - 0s 63ms/step - loss: 2.5191e-05 - val_loss: 2.0710e-05\n",
            "Epoch 132/250\n",
            "8/8 [==============================] - 1s 64ms/step - loss: 2.4789e-05 - val_loss: 2.7516e-05\n",
            "Epoch 133/250\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 2.4155e-05 - val_loss: 1.8465e-05\n",
            "Epoch 134/250\n",
            "8/8 [==============================] - 1s 82ms/step - loss: 2.9559e-05 - val_loss: 1.7986e-05\n",
            "Epoch 135/250\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 3.3962e-05 - val_loss: 4.9494e-05\n",
            "Epoch 136/250\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 3.3068e-05 - val_loss: 4.4920e-05\n",
            "Epoch 137/250\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 3.9487e-05 - val_loss: 6.2525e-05\n",
            "Epoch 138/250\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 3.8906e-05 - val_loss: 7.5513e-05\n",
            "Epoch 139/250\n",
            "8/8 [==============================] - 1s 77ms/step - loss: 3.1712e-05 - val_loss: 3.4219e-05\n",
            "Epoch 140/250\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 2.9653e-05 - val_loss: 1.0493e-04\n",
            "Epoch 141/250\n",
            "8/8 [==============================] - 1s 64ms/step - loss: 3.4274e-05 - val_loss: 1.9656e-05\n",
            "Epoch 142/250\n",
            "8/8 [==============================] - 1s 72ms/step - loss: 2.4426e-05 - val_loss: 1.8632e-05\n",
            "Epoch 143/250\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 2.3013e-05 - val_loss: 1.7436e-05\n",
            "Epoch 144/250\n",
            "8/8 [==============================] - 1s 64ms/step - loss: 2.4190e-05 - val_loss: 5.0751e-05\n",
            "Epoch 145/250\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 3.4949e-05 - val_loss: 4.3672e-05\n",
            "Epoch 146/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 3.2672e-05 - val_loss: 8.4925e-05\n",
            "Epoch 147/250\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 2.6868e-05 - val_loss: 1.9587e-05\n",
            "Epoch 148/250\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 2.4657e-05 - val_loss: 2.8691e-05\n",
            "Epoch 149/250\n",
            "8/8 [==============================] - 0s 61ms/step - loss: 2.6761e-05 - val_loss: 1.8526e-05\n",
            "Epoch 150/250\n",
            "8/8 [==============================] - 1s 64ms/step - loss: 2.5722e-05 - val_loss: 1.7214e-05\n",
            "Epoch 151/250\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 2.4887e-05 - val_loss: 1.8299e-05\n",
            "Epoch 152/250\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 2.4861e-05 - val_loss: 1.9867e-05\n",
            "Epoch 153/250\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 2.2578e-05 - val_loss: 3.2738e-05\n",
            "Epoch 154/250\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 2.2051e-05 - val_loss: 1.9210e-05\n",
            "Epoch 155/250\n",
            "8/8 [==============================] - 1s 64ms/step - loss: 1.8369e-05 - val_loss: 2.0411e-05\n",
            "Epoch 156/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 2.3604e-05 - val_loss: 2.7748e-05\n",
            "Epoch 157/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 2.2001e-05 - val_loss: 2.6186e-05\n",
            "Epoch 158/250\n",
            "8/8 [==============================] - 1s 75ms/step - loss: 2.7983e-05 - val_loss: 2.4553e-05\n",
            "Epoch 159/250\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 2.2684e-05 - val_loss: 2.7849e-05\n",
            "Epoch 160/250\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 2.7240e-05 - val_loss: 1.9606e-05\n",
            "Epoch 161/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 2.1837e-05 - val_loss: 2.1446e-05\n",
            "Epoch 162/250\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 2.9022e-05 - val_loss: 1.9090e-05\n",
            "Epoch 163/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 4.5031e-05 - val_loss: 6.3671e-05\n",
            "Epoch 164/250\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 3.4458e-05 - val_loss: 2.1326e-05\n",
            "Epoch 165/250\n",
            "8/8 [==============================] - 1s 77ms/step - loss: 2.9427e-05 - val_loss: 5.4991e-05\n",
            "Epoch 166/250\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 3.4972e-05 - val_loss: 5.8485e-05\n",
            "Epoch 167/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 2.6927e-05 - val_loss: 2.7757e-05\n",
            "Epoch 168/250\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 2.1338e-05 - val_loss: 1.9566e-05\n",
            "Epoch 169/250\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 2.1301e-05 - val_loss: 3.4949e-05\n",
            "Epoch 170/250\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 2.3640e-05 - val_loss: 4.2075e-05\n",
            "Epoch 171/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 2.2275e-05 - val_loss: 4.5611e-05\n",
            "Epoch 172/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 2.1287e-05 - val_loss: 2.3591e-05\n",
            "Epoch 173/250\n",
            "8/8 [==============================] - 1s 63ms/step - loss: 1.8851e-05 - val_loss: 2.6307e-05\n",
            "Epoch 174/250\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 1.9777e-05 - val_loss: 2.2311e-05\n",
            "Epoch 175/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 2.0099e-05 - val_loss: 2.0262e-05\n",
            "Epoch 176/250\n",
            "8/8 [==============================] - 1s 79ms/step - loss: 2.3225e-05 - val_loss: 1.8636e-05\n",
            "Epoch 177/250\n",
            "8/8 [==============================] - 1s 72ms/step - loss: 2.2784e-05 - val_loss: 2.9250e-05\n",
            "Epoch 178/250\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 2.2370e-05 - val_loss: 2.1484e-05\n",
            "Epoch 179/250\n",
            "8/8 [==============================] - 1s 64ms/step - loss: 2.4544e-05 - val_loss: 3.7756e-05\n",
            "Epoch 180/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 2.3235e-05 - val_loss: 2.7318e-05\n",
            "Epoch 181/250\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 2.0678e-05 - val_loss: 2.1475e-05\n",
            "Epoch 182/250\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 2.1482e-05 - val_loss: 2.3898e-05\n",
            "Epoch 183/250\n",
            "8/8 [==============================] - 1s 72ms/step - loss: 2.1023e-05 - val_loss: 3.5893e-05\n",
            "Epoch 184/250\n",
            "8/8 [==============================] - 1s 72ms/step - loss: 2.1819e-05 - val_loss: 2.1010e-05\n",
            "Epoch 185/250\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 2.3510e-05 - val_loss: 1.9185e-05\n",
            "Epoch 186/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 2.2828e-05 - val_loss: 1.8683e-05\n",
            "Epoch 187/250\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 2.1984e-05 - val_loss: 2.0982e-05\n",
            "Epoch 188/250\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 2.2874e-05 - val_loss: 2.2062e-05\n",
            "Epoch 189/250\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 2.5207e-05 - val_loss: 7.9143e-05\n",
            "Epoch 190/250\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 2.9949e-05 - val_loss: 2.2635e-05\n",
            "Epoch 191/250\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 2.2377e-05 - val_loss: 3.2956e-05\n",
            "Epoch 192/250\n",
            "8/8 [==============================] - 0s 63ms/step - loss: 2.0018e-05 - val_loss: 2.2343e-05\n",
            "Epoch 193/250\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 1.8877e-05 - val_loss: 1.7191e-05\n",
            "Epoch 194/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 1.9481e-05 - val_loss: 4.3135e-05\n",
            "Epoch 195/250\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 2.1273e-05 - val_loss: 1.5546e-05\n",
            "Epoch 196/250\n",
            "8/8 [==============================] - 1s 63ms/step - loss: 2.0236e-05 - val_loss: 3.1135e-05\n",
            "Epoch 197/250\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 2.1967e-05 - val_loss: 1.8619e-05\n",
            "Epoch 198/250\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 2.4617e-05 - val_loss: 5.1196e-05\n",
            "Epoch 199/250\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 3.1158e-05 - val_loss: 3.9992e-05\n",
            "Epoch 200/250\n",
            "8/8 [==============================] - 1s 63ms/step - loss: 3.4468e-05 - val_loss: 5.1768e-05\n",
            "Epoch 201/250\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 2.7494e-05 - val_loss: 3.3328e-05\n",
            "Epoch 202/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 2.0295e-05 - val_loss: 2.4383e-05\n",
            "Epoch 203/250\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 2.3346e-05 - val_loss: 2.1938e-05\n",
            "Epoch 204/250\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 2.1913e-05 - val_loss: 1.9090e-05\n",
            "Epoch 205/250\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 2.0980e-05 - val_loss: 3.9816e-05\n",
            "Epoch 206/250\n",
            "8/8 [==============================] - 1s 64ms/step - loss: 2.4124e-05 - val_loss: 3.1894e-05\n",
            "Epoch 207/250\n",
            "8/8 [==============================] - 1s 63ms/step - loss: 2.6741e-05 - val_loss: 1.0414e-04\n",
            "Epoch 208/250\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 3.0860e-05 - val_loss: 2.6709e-05\n",
            "Epoch 209/250\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 2.5436e-05 - val_loss: 3.1969e-05\n",
            "Epoch 210/250\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 2.2529e-05 - val_loss: 4.1600e-05\n",
            "Epoch 211/250\n",
            "8/8 [==============================] - 1s 62ms/step - loss: 1.8235e-05 - val_loss: 2.1736e-05\n",
            "Epoch 212/250\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 1.8964e-05 - val_loss: 1.9904e-05\n",
            "Epoch 213/250\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 1.8414e-05 - val_loss: 2.2359e-05\n",
            "Epoch 214/250\n",
            "8/8 [==============================] - 1s 74ms/step - loss: 1.9205e-05 - val_loss: 2.9843e-05\n",
            "Epoch 215/250\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 2.4129e-05 - val_loss: 2.1780e-05\n",
            "Epoch 216/250\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 2.1002e-05 - val_loss: 1.6382e-05\n",
            "Epoch 217/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 2.0521e-05 - val_loss: 1.9439e-05\n",
            "Epoch 218/250\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 1.9642e-05 - val_loss: 3.1448e-05\n",
            "Epoch 219/250\n",
            "8/8 [==============================] - 1s 76ms/step - loss: 1.8732e-05 - val_loss: 2.1952e-05\n",
            "Epoch 220/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 1.6717e-05 - val_loss: 2.0137e-05\n",
            "Epoch 221/250\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 1.8918e-05 - val_loss: 2.0639e-05\n",
            "Epoch 222/250\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 2.1041e-05 - val_loss: 2.0688e-05\n",
            "Epoch 223/250\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 1.8120e-05 - val_loss: 3.8664e-05\n",
            "Epoch 224/250\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 1.7866e-05 - val_loss: 1.8298e-05\n",
            "Epoch 225/250\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 1.8632e-05 - val_loss: 2.4106e-05\n",
            "Epoch 226/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 2.4662e-05 - val_loss: 3.3633e-05\n",
            "Epoch 227/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 2.0849e-05 - val_loss: 3.5653e-05\n",
            "Epoch 228/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 1.9062e-05 - val_loss: 2.6893e-05\n",
            "Epoch 229/250\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 1.7157e-05 - val_loss: 2.2500e-05\n",
            "Epoch 230/250\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 1.8882e-05 - val_loss: 2.2034e-05\n",
            "Epoch 231/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 1.8588e-05 - val_loss: 1.9203e-05\n",
            "Epoch 232/250\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 1.8353e-05 - val_loss: 7.4187e-05\n",
            "Epoch 233/250\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 2.3040e-05 - val_loss: 2.0408e-05\n",
            "Epoch 234/250\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 1.8588e-05 - val_loss: 2.8971e-05\n",
            "Epoch 235/250\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 2.2041e-05 - val_loss: 2.1449e-05\n",
            "Epoch 236/250\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 2.3591e-05 - val_loss: 5.9266e-05\n",
            "Epoch 237/250\n",
            "8/8 [==============================] - 1s 64ms/step - loss: 2.6329e-05 - val_loss: 8.3895e-05\n",
            "Epoch 238/250\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 2.1250e-05 - val_loss: 2.4321e-05\n",
            "Epoch 239/250\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 1.9763e-05 - val_loss: 2.9314e-05\n",
            "Epoch 240/250\n",
            "8/8 [==============================] - 1s 77ms/step - loss: 3.4676e-05 - val_loss: 2.9696e-05\n",
            "Epoch 241/250\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 2.6661e-05 - val_loss: 3.3782e-05\n",
            "Epoch 242/250\n",
            "8/8 [==============================] - 1s 64ms/step - loss: 2.8549e-05 - val_loss: 2.3736e-05\n",
            "Epoch 243/250\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 2.3378e-05 - val_loss: 2.7659e-05\n",
            "Epoch 244/250\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 2.0143e-05 - val_loss: 1.9775e-05\n",
            "Epoch 245/250\n",
            "8/8 [==============================] - 1s 64ms/step - loss: 2.0651e-05 - val_loss: 3.9589e-05\n",
            "Epoch 246/250\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 1.7161e-05 - val_loss: 3.2945e-05\n",
            "Epoch 247/250\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 1.9382e-05 - val_loss: 5.6757e-05\n",
            "Epoch 248/250\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 1.9544e-05 - val_loss: 4.3040e-05\n",
            "Epoch 249/250\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 1.9248e-05 - val_loss: 1.8127e-05\n",
            "Epoch 250/250\n",
            "8/8 [==============================] - 1s 86ms/step - loss: 1.9323e-05 - val_loss: 4.6587e-05\n",
            "Training loss\n",
            "{'loss': [0.004951180424541235, 0.0011444370029494166, 0.0006882944144308567, 0.0005705903167836368, 0.0005590500077232718, 0.0005021037650294602, 0.0004976341151632369, 0.0003927513025701046, 0.0002981763973366469, 0.0001811549736885354, 0.0001223614817718044, 0.00010097898484673351, 8.152642840286717e-05, 7.247383473441005e-05, 7.308609929168597e-05, 6.794666114728898e-05, 6.540786853292957e-05, 6.184971425682306e-05, 6.64679755573161e-05, 8.000851084943861e-05, 6.425913306884468e-05, 7.143794937292114e-05, 6.788220343878493e-05, 5.749277988797985e-05, 5.69813528272789e-05, 5.7557983382139355e-05, 5.5846150644356385e-05, 5.1415736379567534e-05, 4.821845141123049e-05, 4.8088437324622646e-05, 5.7167490012943745e-05, 5.6419627071591094e-05, 4.9633465096121654e-05, 4.593485937220976e-05, 4.902377258986235e-05, 4.157091461820528e-05, 4.440523844095878e-05, 3.709891825565137e-05, 3.611738429754041e-05, 3.7284069549059495e-05, 4.090380753041245e-05, 3.787903187912889e-05, 3.868040948873386e-05, 6.742364348610863e-05, 5.405470801633783e-05, 4.6302178816404194e-05, 4.604335845215246e-05, 3.77044161723461e-05, 4.124187398701906e-05, 3.799981277552433e-05, 3.905512858182192e-05, 4.0734867070568725e-05, 3.751903568627313e-05, 3.737259976333007e-05, 5.5698019423289225e-05, 4.041480497107841e-05, 3.6688841646537185e-05, 3.614026718423702e-05, 4.330949013819918e-05, 3.6391178582562134e-05, 3.4814165701391175e-05, 3.775319783017039e-05, 3.32306299242191e-05, 2.731198947003577e-05, 3.221754013793543e-05, 3.155679951305501e-05, 3.671666854643263e-05, 3.498404475976713e-05, 3.797081808443181e-05, 3.283602563897148e-05, 3.273025504313409e-05, 4.649413313018158e-05, 3.570362605387345e-05, 4.4614513171836734e-05, 3.4162771044066176e-05, 3.304868005216122e-05, 3.190037750755437e-05, 3.191369614796713e-05, 3.522653059917502e-05, 2.995612521772273e-05, 4.6592111175414175e-05, 4.000455010100268e-05, 3.075081986025907e-05, 3.7408161006169394e-05, 3.5509467124938965e-05, 2.897622289310675e-05, 2.8951997592230327e-05, 4.123901089769788e-05, 3.0967734346631914e-05, 3.4446002246113494e-05, 4.255043313605711e-05, 3.430230208323337e-05, 3.5176341043552384e-05, 3.51049420714844e-05, 3.434828977333382e-05, 3.708396616275422e-05, 3.3734439057298005e-05, 3.071405808441341e-05, 2.5876250219880603e-05, 2.5990782887674868e-05, 3.065016062464565e-05, 3.4120555937988684e-05, 3.0500339562422596e-05, 2.60112155956449e-05, 2.956665775855072e-05, 3.380023917998187e-05, 3.1656254577683285e-05, 4.566955249174498e-05, 3.0216009690775536e-05, 3.2576790545135736e-05, 3.309193198219873e-05, 2.4082442905637436e-05, 2.600310654088389e-05, 2.394919647485949e-05, 2.3940969185787253e-05, 3.316421862109564e-05, 2.7586811484070495e-05, 3.643761738203466e-05, 3.10119976347778e-05, 3.397382897674106e-05, 3.822139842668548e-05, 3.469036164460704e-05, 2.845431663445197e-05, 4.315422484069131e-05, 3.230186484870501e-05, 4.489911225391552e-05, 3.829865454463288e-05, 3.3947042538784444e-05, 2.6049932785099372e-05, 2.481188676028978e-05, 2.5191131499013864e-05, 2.478912938386202e-05, 2.4155204300768673e-05, 2.9559074391727336e-05, 3.396228203200735e-05, 3.306765574961901e-05, 3.9487080357503146e-05, 3.8905563997104764e-05, 3.171205389662646e-05, 2.965274325106293e-05, 3.4273940400453284e-05, 2.442629192955792e-05, 2.3012918973108754e-05, 2.4189930627471767e-05, 3.494871634757146e-05, 3.2671836379449815e-05, 2.6868086933973245e-05, 2.465653778926935e-05, 2.67611558228964e-05, 2.5722210921230726e-05, 2.488714198989328e-05, 2.4861268684617244e-05, 2.257812775496859e-05, 2.2050668121664785e-05, 1.8368655219092034e-05, 2.3603726731380448e-05, 2.2000693206791766e-05, 2.7982536266790703e-05, 2.268413118144963e-05, 2.7239644623477943e-05, 2.1837386157130823e-05, 2.902207779698074e-05, 4.503110176301561e-05, 3.445782567723654e-05, 2.9426610126392916e-05, 3.49721776728984e-05, 2.692664202186279e-05, 2.1338433725759387e-05, 2.130142274836544e-05, 2.364032479817979e-05, 2.2274647562881e-05, 2.1286934497766197e-05, 1.8851298591471277e-05, 1.9777402485487983e-05, 2.0098786990274675e-05, 2.3225167751661502e-05, 2.2784433895139955e-05, 2.2370300939655863e-05, 2.4543867766624317e-05, 2.3234639229485765e-05, 2.0677685824921355e-05, 2.1481551812030375e-05, 2.102309008478187e-05, 2.1819243556819856e-05, 2.3510499886469916e-05, 2.2828475266578607e-05, 2.1984251361573115e-05, 2.287401548528578e-05, 2.5206618374795653e-05, 2.994858914462384e-05, 2.2377482309821062e-05, 2.001786560867913e-05, 1.8876608010032214e-05, 1.9481269191601314e-05, 2.1272559024509974e-05, 2.0235876945662312e-05, 2.1967473003314808e-05, 2.4616656446596608e-05, 3.1158058845903724e-05, 3.44676009262912e-05, 2.7493995730765164e-05, 2.029478309850674e-05, 2.3345661247731186e-05, 2.1912828742642887e-05, 2.098010736517608e-05, 2.412376670690719e-05, 2.6741092369775288e-05, 3.086025026277639e-05, 2.5436396754230373e-05, 2.252948070236016e-05, 1.823533784772735e-05, 1.8963908587465994e-05, 1.841366793087218e-05, 1.920518116094172e-05, 2.41286452364875e-05, 2.1002371795475483e-05, 2.05207852559397e-05, 1.9642375264083967e-05, 1.8731818272499368e-05, 1.67172711371677e-05, 1.8918441128334962e-05, 2.1040530555183068e-05, 1.8120359527529217e-05, 1.7865955669549294e-05, 1.8632379578775726e-05, 2.4662249415996484e-05, 2.084881634800695e-05, 1.9062008504988626e-05, 1.7157224647235125e-05, 1.8882128642871976e-05, 1.8588321836432442e-05, 1.835305920394603e-05, 2.304011650267057e-05, 1.8587932572700083e-05, 2.2041314878151752e-05, 2.3591028366354294e-05, 2.6328832973376848e-05, 2.125005812558811e-05, 1.9763143427553587e-05, 3.467553688096814e-05, 2.6660522053134628e-05, 2.8549238777486607e-05, 2.3377706384053454e-05, 2.0143197616562247e-05, 2.0650548322009854e-05, 1.7160651623271406e-05, 1.9382136088097468e-05, 1.9543731468729675e-05, 1.9248358512413688e-05, 1.9322780644870363e-05], 'val_loss': [0.0047401925548911095, 0.0029807917308062315, 0.003995900042355061, 0.003924199845641851, 0.003232031362131238, 0.0026952235493808985, 0.0026263624895364046, 0.0015328635927289724, 0.0007623483543284237, 0.0004238573310431093, 0.00032725243363529444, 8.673347474541515e-05, 0.00016779970610514283, 0.00034539998159743845, 0.00011510754120536149, 7.240723061840981e-05, 0.0001043001320795156, 6.01091560383793e-05, 2.9685514164157212e-05, 4.5252996642375365e-05, 8.187294588424265e-05, 2.401968049525749e-05, 6.641301297349855e-05, 7.551187445642427e-05, 0.00010308775381417945, 0.00016483398212585598, 4.399925092002377e-05, 4.37226626672782e-05, 0.0001013085784506984, 0.00013425041106529534, 7.583208935102448e-05, 8.795596659183502e-05, 6.681056402157992e-05, 0.00011805800750153139, 5.1558748964453116e-05, 5.8072051615454257e-05, 3.408981501706876e-05, 4.271784200682305e-05, 6.346347072394565e-05, 4.1051622247323394e-05, 3.749887764570303e-05, 5.201856401981786e-05, 3.376110180397518e-05, 9.096868598135188e-05, 0.00020056807261426002, 2.6179070118814707e-05, 8.545217133359984e-05, 4.648231697501615e-05, 2.8976959583815187e-05, 3.134281359962188e-05, 4.6752236812608317e-05, 7.958089554449543e-05, 2.2622078176937066e-05, 4.063758751726709e-05, 7.915851892903447e-05, 3.161288987030275e-05, 2.7685286113410257e-05, 5.522184073925018e-05, 4.3772266508312896e-05, 4.805745629710145e-05, 6.668183050351217e-05, 2.3049833544064313e-05, 8.110175986075774e-05, 2.7782512916019186e-05, 5.6751152442302555e-05, 2.576683255028911e-05, 2.7392856281949207e-05, 2.475835572113283e-05, 3.256529089412652e-05, 4.2050342017319053e-05, 6.998963363002986e-05, 6.485281483037397e-05, 0.00017378199845552444, 2.44140210270416e-05, 3.003076017193962e-05, 5.795143442810513e-05, 8.054923819145188e-05, 2.6357358365203254e-05, 4.198676106170751e-05, 3.3806154533522204e-05, 0.00021568989905063063, 2.9123371859895997e-05, 3.137308522127569e-05, 8.077449456322938e-05, 4.88790356030222e-05, 3.098067099926993e-05, 8.178680582204834e-05, 8.071230695350096e-05, 2.7257639885647222e-05, 5.181294181966223e-05, 2.170750667573884e-05, 3.200788705726154e-05, 2.01755938178394e-05, 3.0486296964227222e-05, 5.246489308774471e-05, 5.697206506738439e-05, 2.128097003151197e-05, 2.415074959571939e-05, 2.2014506612322293e-05, 3.895843474310823e-05, 7.581176760140806e-05, 3.0132756364764646e-05, 3.105582800344564e-05, 3.4727454476524144e-05, 1.7343747458653525e-05, 2.787399171211291e-05, 5.678398520103656e-05, 2.207914258178789e-05, 2.2778134734835476e-05, 5.1886934670619667e-05, 1.741101914376486e-05, 2.229533129138872e-05, 2.1372396076912992e-05, 1.9585811969591305e-05, 2.473345557518769e-05, 4.956149132340215e-05, 1.7547568859299645e-05, 9.854970267042518e-05, 6.010139986756258e-05, 0.0002665241190697998, 4.647605965146795e-05, 4.003135836683214e-05, 4.4934018660569564e-05, 5.256659642327577e-05, 0.00022149053984321654, 3.0052189686102793e-05, 5.007327126804739e-05, 1.725128822727129e-05, 1.907534351630602e-05, 2.7683638109010644e-05, 2.0710369426524267e-05, 2.7515863621374592e-05, 1.846469604060985e-05, 1.7985928934649564e-05, 4.949371577822603e-05, 4.4920096115674824e-05, 6.252456660149619e-05, 7.55127111915499e-05, 3.421871952014044e-05, 0.00010492868022993207, 1.9655892174341716e-05, 1.8632445062394254e-05, 1.7436213965993375e-05, 5.0751306844176725e-05, 4.367191650089808e-05, 8.492473716614768e-05, 1.9586786947911605e-05, 2.8690583349089138e-05, 1.8525861378293484e-05, 1.7213817045558244e-05, 1.8299182556802407e-05, 1.986720963031985e-05, 3.2737621950218454e-05, 1.9210057871532626e-05, 2.0411065634107217e-05, 2.774831409624312e-05, 2.6186029572272673e-05, 2.4553111870773137e-05, 2.7849333491758443e-05, 1.960570261871908e-05, 2.1446316168294288e-05, 1.908962440211326e-05, 6.367143214447424e-05, 2.1325869965949096e-05, 5.499098915606737e-05, 5.848516957485117e-05, 2.7756999770645052e-05, 1.9565706679713912e-05, 3.494942939141765e-05, 4.207491292618215e-05, 4.561106470646337e-05, 2.3590704586240463e-05, 2.6306917789042927e-05, 2.2311245629680343e-05, 2.0261977624613792e-05, 1.8635933884070255e-05, 2.925009175669402e-05, 2.1484343960764818e-05, 3.775624281843193e-05, 2.7317963031237014e-05, 2.147536724805832e-05, 2.3898166546132416e-05, 3.589273546822369e-05, 2.101046447933186e-05, 1.918452835525386e-05, 1.868264371296391e-05, 2.098197910527233e-05, 2.2061631170799956e-05, 7.914343586890027e-05, 2.2634960259892978e-05, 3.295574424555525e-05, 2.2342765078064986e-05, 1.719087413221132e-05, 4.3134783481946215e-05, 1.554554000904318e-05, 3.1135092285694554e-05, 1.8619461116031744e-05, 5.1195940613979474e-05, 3.99921482312493e-05, 5.176841659704223e-05, 3.332829874125309e-05, 2.4383225536439568e-05, 2.1938201825832948e-05, 1.90903629118111e-05, 3.981599002145231e-05, 3.189380367984995e-05, 0.00010413621930638328, 2.6708619770943187e-05, 3.1969091651262715e-05, 4.1600163967814296e-05, 2.1735928385169245e-05, 1.9903713109670207e-05, 2.2358999558491632e-05, 2.9843120501027443e-05, 2.1779713279101998e-05, 1.6381725799874403e-05, 1.943889401445631e-05, 3.144788206554949e-05, 2.195159140683245e-05, 2.013728408201132e-05, 2.0639432477764785e-05, 2.06877120945137e-05, 3.8664238672936335e-05, 1.8298323993803933e-05, 2.4105542252073064e-05, 3.3632990380283445e-05, 3.5653109080158174e-05, 2.689263055799529e-05, 2.249978933832608e-05, 2.2033584173186682e-05, 1.9202776456950232e-05, 7.418666064040735e-05, 2.040762956312392e-05, 2.897076410590671e-05, 2.1448968254844658e-05, 5.926620360696688e-05, 8.389496360905468e-05, 2.4321469027199782e-05, 2.9313567210920155e-05, 2.9695544071728364e-05, 3.3782423997763544e-05, 2.3736005459795706e-05, 2.7658532417262904e-05, 1.977533975150436e-05, 3.958894740208052e-05, 3.294507769169286e-05, 5.675686043105088e-05, 4.304035974200815e-05, 1.8126951545127667e-05, 4.658711623051204e-05]}\n",
            "MSE of Test dataset\n",
            "0.01007418426870799\n",
            "Predicted output\n",
            "[[0.00700548 0.10519515 0.00158156 0.03495641 0.07670592 0.00657564\n",
            "  0.00995684 0.1018586  0.17332534 0.04336952 0.3735202  0.06419504]]\n",
            "Actual output\n",
            "[[0.         0.10545455 0.00145455 0.048      0.08218182 0.\n",
            "  0.00145455 0.10036364 0.17454545 0.03927273 0.37672727 0.07054545]]\n",
            "MSE of prediction\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.006027226047868324"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    }
  ]
}