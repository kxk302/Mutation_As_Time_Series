{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mutation_As_Time_Series",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOxzn9NFz7v6GDWMFXw3ymf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kxk302/Mutation_As_Time_Series/blob/main/Mutation_As_Time_Series.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwCiqdt8a0bt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8053f216-4e22-406e-eb75-36db8117fdea"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu33b9McbRmn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63f31589-16ee-47d0-e8be-d4c468601d0d"
      },
      "source": [
        "!ls '/content/gdrive/MyDrive/Colab Notebooks/data'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_sorted.tsv  data.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1pO3FlQbWvr"
      },
      "source": [
        "#\n",
        "# Sort the dataset based on Collection_Date and Sample, in ascending order\n",
        "#\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from dateutil import parser\n",
        "\n",
        "pd.set_option('max_rows', None)\n",
        "\n",
        "df_in = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/data/data.tsv', sep='\\t', names=['Sample', 'Collection_Date', 'UNK1', 'UNK2', 'UNK3', 'POS', 'REF', 'ALT', 'EFFECT', 'CODON', 'TRID', 'AA', 'AF'])\n",
        "\n",
        "# DEBUG\n",
        "# df_in = df_in.iloc[:1000,:]\n",
        "\n",
        "print(type(df_in.Collection_Date[0]))\n",
        "df_in.Collection_Date = df_in.Collection_Date.apply(lambda x: parser.parse(x))\n",
        "print(type(df_in.Collection_Date[0]))\n",
        "\n",
        "df_in.sort_values(by=['Collection_Date', 'Sample'], ascending=[True, True], inplace=True)\n",
        "print('\\n\\n')\n",
        "print(df_in.shape)\n",
        "print(df_in.head(1000))\n",
        "\n",
        "# df_in.to_csv('/content/gdrive/MyDrive/Colab Notebooks/data/data_sorted.tsv', sep='\\t', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EZdu7nDm9BE"
      },
      "source": [
        "#\n",
        "# Filter the dataset, calculate normalized mutation counts, pivot the data, and save to file\n",
        "#\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from dateutil import parser\n",
        "\n",
        "pd.set_option('max_rows', None)\n",
        "pd.set_option('max_columns', None)\n",
        "\n",
        "start_date = '2021-01-27'\n",
        "end_date = '2021-08-09'\n",
        "file_name = '/content/gdrive/MyDrive/Colab Notebooks/data/data_sorted.tsv'\n",
        "separator = '\\t'\n",
        "# NON_SYNONYMOUS_CODING: missense mutation\n",
        "effects = ['NON_SYNONYMOUS_CODING']\n",
        "drop_columns = ['UNK1', 'UNK2', 'UNK3']\n",
        "\n",
        "# Index of nucleotide changes\n",
        "#\n",
        "# 0: A->T\n",
        "# 1: A->G\n",
        "# 2: A->C\n",
        "#\n",
        "# 3: T->G\n",
        "# 4: T->C\n",
        "# 5: T->A\n",
        "#\n",
        "# 6: G->C\n",
        "# 7: G->A\n",
        "# 8: G->T\n",
        "#\n",
        "# 9:  C->A\n",
        "# 10: C->T\n",
        "# 11: C->G\n",
        "#\n",
        "atgc_dict = {'A': {'T': 0, 'G': 1, 'C': 2}, 'T': {'G': 3, 'C': 4, 'A': 5}, 'G': {'C': 6, 'A': 7, 'T': 8}, 'C': {'A': 9, 'T':10, 'G': 11}}\n",
        "\n",
        "# Read the input file\n",
        "df_in = pd.read_csv(file_name, sep=separator)\n",
        "\n",
        "# Drop the unnecessary columns\n",
        "df_in.drop(columns=drop_columns, inplace=True)\n",
        "\n",
        "# Select only rows with mutation type specified in 'effects' list\n",
        "df_eff = df_in[ df_in.EFFECT.isin(effects) ]\n",
        "\n",
        "# Select only rows where the Collection_Date falls between start_date and end_date \n",
        "df_fil = df_eff[ (df_eff.Collection_Date >= start_date) & (df_eff.Collection_Date <= end_date) ]\n",
        "print('\\n\\n')\n",
        "print('Filtered df shape {}'.format(df_fil.shape))\n",
        "print('Number of unique dates {}'.format(len(df_fil.Collection_Date.unique())))\n",
        " \n",
        "df_fil['nucleotide_change'] = df_fil.apply(lambda x: atgc_dict[x.REF][x.ALT], axis=1)\n",
        "print('Calculating normalize_nucleotide_change')\n",
        "normalize_nucleotide_change = df_fil.groupby(df_fil.Collection_Date).nucleotide_change.value_counts() / df_fil.groupby(df_fil.Collection_Date).nucleotide_change.count()\n",
        "print('\\n\\n')\n",
        "print('Type of normalize_nucleotide_change')\n",
        "print(type(normalize_nucleotide_change))\n",
        "print('Name of normalize_nucleotide_change BEFORE rename')\n",
        "print(normalize_nucleotide_change.name)\n",
        "normalize_nucleotide_change = normalize_nucleotide_change.rename( 'normalize_nucleotide_change')\n",
        "print('Name of normalize_nucleotide_change AFTER rename')\n",
        "print(normalize_nucleotide_change.name)\n",
        "print('Index of normalize_nucleotide_change')\n",
        "print(normalize_nucleotide_change.index)\n",
        "\n",
        "# Convert normalize_nucleotide_change Series to Dataframe\n",
        "df = normalize_nucleotide_change.to_frame()\n",
        "\n",
        "print('\\n\\n')\n",
        "print('Type of df')\n",
        "print(type(df))\n",
        "print('Columns of df')\n",
        "print(df.columns)\n",
        "print('df.head(5)')\n",
        "print(df.head(5))\n",
        "print('df.index BEFORE reset')\n",
        "print(df.index)\n",
        "df.reset_index(inplace=True)\n",
        "print('df.index AFTER reset')\n",
        "print(df.index)\n",
        "print('df.head(5) AFTER reset')\n",
        "print(df.head(25))\n",
        "\n",
        "df_piv = pd.pivot_table(df, index='Collection_Date', columns='nucleotide_change', values='normalize_nucleotide_change')\n",
        "df_piv.fillna(0, inplace=True)\n",
        "print('\\n\\n')\n",
        "print('Pivoted df')\n",
        "print(df_piv.head(5))\n",
        "print('Index of df_piv')\n",
        "print(df_piv.index)\n",
        "print('Columns of df_piv')\n",
        "print(df_piv.columns)\n",
        "print('Shape of df_piv')\n",
        "print(df_piv.shape)\n",
        "\n",
        "df_piv.to_csv('/content/gdrive/MyDrive/Colab Notebooks/data/data_pivoted.tsv', sep=separator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yYlq1g82id2"
      },
      "source": [
        "#\n",
        "# Create time series prediction dataset from pivoted data\n",
        "#\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from dateutil import parser\n",
        "\n",
        "pd.set_option('max_rows', None)\n",
        "pd.set_option('max_columns', None)\n",
        "\n",
        "file_name = '/content/gdrive/MyDrive/Colab Notebooks/data/data_pivoted.tsv'\n",
        "separator = '\\t'\n",
        "train_data_percentage = 0.80\n",
        "\n",
        "# Read the input file\n",
        "df_in = pd.read_csv(file_name, sep=separator)\n",
        "\n",
        "# Drop the Collection_Date column\n",
        "df_in.drop(columns='Collection_Date', inplace=True)\n",
        "\n",
        "print(df_in.head(5))\n",
        "print(df_in.shape[0])\n",
        "print(df_in.shape[1])\n",
        "\n",
        "trainX = []\n",
        "trainY = []\n",
        "\n",
        "num_future = 1\n",
        "num_past = 14\n",
        "num_rows = df_in.shape[0]\n",
        "num_cols = df_in.shape[1]\n",
        "\n",
        "for i in range(num_past, num_rows - num_future + 1):\n",
        "  trainX.append(df_in.iloc[i - num_past:i, 0:num_cols])\n",
        "  trainY.append(df_in.iloc[i + num_future - 1:i + num_future, 0:num_cols])\n",
        "\n",
        "trainX, trainY = np.array(trainX), np.array(trainY)\n",
        "\n",
        "print('trainX shape == {}'.format(trainX.shape))\n",
        "print('trainY shape == {}'.format(trainY.shape))\n",
        "\n",
        "# trainX = trainX.reshape(trainX.shape[0], trainX.shape[1] * trainX.shape[2])\n",
        "trainY = trainY.reshape(trainY.shape[0], trainY.shape[1] * trainY.shape[2])\n",
        "\n",
        "# print('trainX shape == {}'.format(trainX.shape))\n",
        "print('trainY shape == {}'.format(trainY.shape))\n",
        "\n",
        "train_data_idx = int(trainX.shape[0] * train_data_percentage)\n",
        "\n",
        "testX = trainX[train_data_idx:,:,:]\n",
        "testY = trainY[train_data_idx:,:]\n",
        "trainX = trainX[0:train_data_idx,:,:]\n",
        "trainY = trainY[0:train_data_idx,:]\n",
        "\n",
        "print('trainX shape == {}'.format(trainX.shape))\n",
        "print('trainY shape == {}'.format(trainY.shape))\n",
        "print('testX shape == {}'.format(testX.shape))\n",
        "print('testY shape == {}'.format(testY.shape))\n",
        "\n",
        "'''\n",
        "np.savetxt('/content/gdrive/MyDrive/Colab Notebooks/data/trainX.tsv', trainX, delimiter=separator)\n",
        "np.savetxt('/content/gdrive/MyDrive/Colab Notebooks/data/trainY.tsv', trainY, delimiter=separator)\n",
        "np.savetxt('/content/gdrive/MyDrive/Colab Notebooks/data/testX.tsv', testX, delimiter=separator)\n",
        "np.savetxt('/content/gdrive/MyDrive/Colab Notebooks/data/testY.tsv', testY, delimiter=separator)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c7WB0W3GBH8"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, LSTM\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(500, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences=True))\n",
        "model.add(Dense(250, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(150, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(trainY.shape[1]))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(trainX, trainY, epochs=100, batch_size=15, validation_split=0.2, verbose=1)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}