{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mutation_As_Time_Series",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOrG7S8EJhiJ0dkLhRKsg+G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kxk302/Mutation_As_Time_Series/blob/main/Mutation_As_Time_Series.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwCiqdt8a0bt"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu33b9McbRmn"
      },
      "source": [
        "!ls '/content/gdrive/MyDrive/Colab Notebooks/Mutation_As_Time_Series_Folder'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1pO3FlQbWvr"
      },
      "source": [
        "#\n",
        "# ONLY need to run this ONCE\n",
        "# Sort the dataset based on Collection_Date and Sample, in ascending order\n",
        "#\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from dateutil import parser\n",
        "\n",
        "pd.set_option('max_rows', None)\n",
        "\n",
        "path = '/content/gdrive/MyDrive/Colab Notebooks/Mutation_As_Time_Series_Folder'\n",
        "\n",
        "df_in = pd.read_csv(path + '/data/data.tsv', sep='\\t', names=['Sample', 'Collection_Date', 'UNK1', 'UNK2', 'UNK3', 'POS', 'REF', 'ALT', 'EFFECT', 'CODON', 'TRID', 'AA', 'AF'])\n",
        "\n",
        "print(type(df_in.Collection_Date[0]))\n",
        "df_in.Collection_Date = df_in.Collection_Date.apply(lambda x: parser.parse(x))\n",
        "print(type(df_in.Collection_Date[0]))\n",
        "\n",
        "df_in.sort_values(by=['Collection_Date', 'Sample'], ascending=[True, True], inplace=True)\n",
        "print('\\n\\n')\n",
        "print(df_in.shape)\n",
        "print(df_in.head(1000))\n",
        "\n",
        "# df_in.to_csv(path + '/data/data_sorted.tsv', sep='\\t', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EZdu7nDm9BE"
      },
      "source": [
        "#\n",
        "# ONLY need to run this if changing the filtering criteria like start/end dates, or effects\n",
        "# Filter the dataset, calculate normalized mutation counts, pivot the data, and save to file\n",
        "#\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from dateutil import parser\n",
        "\n",
        "pd.set_option('max_rows', None)\n",
        "pd.set_option('max_columns', None)\n",
        "\n",
        "start_date = '2020-04-13'\n",
        "end_date = '2021-08-09'\n",
        "path = '/content/gdrive/MyDrive/Colab Notebooks/Mutation_As_Time_Series_Folder'\n",
        "file_name = path + '/data/data_sorted.tsv'\n",
        "separator = '\\t'\n",
        "effects = ['NON_SYNONYMOUS_CODING'] # NON_SYNONYMOUS_CODING -> missense mutation\n",
        "drop_columns = ['UNK1', 'UNK2', 'UNK3']\n",
        "atgc_dict = {'A': {'T': 0, 'G': 1, 'C': 2}, 'T': {'G': 3, 'C': 4, 'A': 5}, 'G': {'C': 6, 'A': 7, 'T': 8}, 'C': {'A': 9, 'T':10, 'G': 11}}\n",
        "\n",
        "# Index of nucleotide changes\n",
        "#\n",
        "# 0: A->T\n",
        "# 1: A->G\n",
        "# 2: A->C\n",
        "#\n",
        "# 3: T->G\n",
        "# 4: T->C\n",
        "# 5: T->A\n",
        "#\n",
        "# 6: G->C\n",
        "# 7: G->A\n",
        "# 8: G->T\n",
        "#\n",
        "# 9:  C->A\n",
        "# 10: C->T\n",
        "# 11: C->G\n",
        "#\n",
        "\n",
        "# Read the input file\n",
        "df_in = pd.read_csv(file_name, sep=separator)\n",
        "\n",
        "# Drop the unnecessary columns\n",
        "df_in.drop(columns=drop_columns, inplace=True)\n",
        "\n",
        "# Select only rows with mutation type specified in 'effects' list\n",
        "df_eff = df_in[ df_in.EFFECT.isin(effects) ]\n",
        "\n",
        "# Select only rows where the Collection_Date falls between start_date and end_date \n",
        "df_fil = df_eff[ (df_eff.Collection_Date >= start_date) & (df_eff.Collection_Date <= end_date) ]\n",
        "\n",
        "print('\\n\\n')\n",
        "print('Filtered df shape {}'.format(df_fil.shape))\n",
        "print('Number of unique dates {}'.format(len(df_fil.Collection_Date.unique())))\n",
        "\n",
        "print('Calculating nucleotide_change') \n",
        "df_fil['nucleotide_change'] = df_fil.apply(lambda x: atgc_dict[x.REF][x.ALT], axis=1)\n",
        "\n",
        "print('Calculating normalized_nucleotide_change')\n",
        "normalized_nucleotide_change = df_fil.groupby(df_fil.Collection_Date).nucleotide_change.value_counts() / df_fil.groupby(df_fil.Collection_Date).nucleotide_change.count()\n",
        "\n",
        "print('\\n\\n')\n",
        "print('Type of normalized_nucleotide_change')\n",
        "print(type(normalized_nucleotide_change))\n",
        "\n",
        "# Rename to avoid clash when resetting index\n",
        "print('Name of normalized_nucleotide_change BEFORE rename')\n",
        "print(normalized_nucleotide_change.name)\n",
        "normalized_nucleotide_change = normalized_nucleotide_change.rename( 'normalized_nucleotide_change')\n",
        "print('Name of normalized_nucleotide_change AFTER rename')\n",
        "print(normalized_nucleotide_change.name)\n",
        "print('Index of normalized_nucleotide_change')\n",
        "print(normalized_nucleotide_change.index)\n",
        "\n",
        "# Convert normalized_nucleotide_change Series to Dataframe\n",
        "df = normalized_nucleotide_change.to_frame()\n",
        "\n",
        "print('\\n\\n')\n",
        "print('Type of df')\n",
        "print(type(df))\n",
        "print('Columns of df')\n",
        "print(df.columns)\n",
        "print('df.head(5)')\n",
        "print(df.head(5))\n",
        "\n",
        "print('df.index BEFORE reset')\n",
        "print(df.index)\n",
        "df.reset_index(inplace=True)\n",
        "print('df.index AFTER reset')\n",
        "print(df.index)\n",
        "print('df.head(5) AFTER reset')\n",
        "print(df.head(5))\n",
        "\n",
        "# Pivot the data \n",
        "df_piv = pd.pivot_table(df, index='Collection_Date', columns='nucleotide_change', values='normalized_nucleotide_change')\n",
        "df_piv.fillna(0, inplace=True)\n",
        "\n",
        "print('\\n\\n')\n",
        "print('Pivoted df')\n",
        "print(df_piv.head(5))\n",
        "print('Index of df_piv')\n",
        "print(df_piv.index)\n",
        "print('Columns of df_piv')\n",
        "print(df_piv.columns)\n",
        "print('Shape of df_piv')\n",
        "print(df_piv.shape)\n",
        "\n",
        "df_piv.to_csv(path + '/data/data_pivoted_' + start_date + '_' + end_date + '.tsv', sep=separator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yYlq1g82id2"
      },
      "source": [
        "#\n",
        "# Create time series prediction dataset from pivoted data\n",
        "# Depends on how many time steps in the past we look, \n",
        "# and how many timestamps in the future we predict\n",
        "#\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from dateutil import parser\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "pd.set_option('max_rows', None)\n",
        "pd.set_option('max_columns', None)\n",
        "\n",
        "def create_time_series_dataset(start_date = '2020-04-13', \n",
        "                               end_date = '2021-08-09',\n",
        "                               path = '/content/gdrive/MyDrive/Colab Notebooks/Mutation_As_Time_Series_Folder',\n",
        "                               data_folder = '/data', \n",
        "                               separator = '\\t',\n",
        "                               test_data_percentage = 0.20,\n",
        "                               num_future = 1,\n",
        "                               num_past = 14):\n",
        "  \n",
        "  file_name = path + data_folder + '/data_pivoted_' + start_date + '_' + end_date + '.tsv'\n",
        "\n",
        "  # Read the input file\n",
        "  df_in = pd.read_csv(file_name, sep=separator)\n",
        "\n",
        "  # Drop the Collection_Date column\n",
        "  df_in.drop(columns='Collection_Date', inplace=True)\n",
        "\n",
        "  print(df_in.head(5))\n",
        "  print(df_in.shape)\n",
        "\n",
        "  X = []\n",
        "  Y = []\n",
        "\n",
        "  num_rows = df_in.shape[0]\n",
        "  num_cols = df_in.shape[1]\n",
        "\n",
        "  for i in range(num_past, num_rows - num_future + 1):\n",
        "    X.append(df_in.iloc[i - num_past:i, 0:num_cols])\n",
        "    Y.append(df_in.iloc[i + num_future - 1:i + num_future, 0:num_cols])\n",
        "\n",
        "  X, Y = np.array(X), np.array(Y)\n",
        "\n",
        "  print('X shape == {}'.format(X.shape))\n",
        "  print('Y shape == {}'.format(Y.shape))\n",
        "\n",
        "  X_shape_1 = X.shape[1]\n",
        "  X_shape_2 = X.shape[2]\n",
        "\n",
        "  # Must reshape as such, so splitting the data into train/test makes sense\n",
        "  X = X.reshape(X.shape[0], X.shape[1] * X.shape[2])\n",
        "  Y = Y.reshape(Y.shape[0], Y.shape[1] * Y.shape[2])\n",
        "  print('X shape == {}'.format(X.shape))\n",
        "  print('Y shape == {}'.format(Y.shape))\n",
        "\n",
        "  trainX, testX, trainY, testY = train_test_split(X, Y, test_size=test_data_percentage)\n",
        "\n",
        "  # Reshape to the original shape\n",
        "  trainX = trainX.reshape(trainX.shape[0], X_shape_1, X_shape_2)\n",
        "  testX = testX.reshape(testX.shape[0], X_shape_1, X_shape_2)\n",
        "\n",
        "  print('trainX shape == {}'.format(trainX.shape))\n",
        "  print('trainY shape == {}'.format(trainY.shape))\n",
        "  print('testX shape == {}'.format(testX.shape))\n",
        "  print('testY shape == {}'.format(testY.shape))\n",
        "\n",
        "  return trainX, testX, trainY, testY"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c7WB0W3GBH8"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, LSTM\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "path = '/content/gdrive/MyDrive/Colab Notebooks/Mutation_As_Time_Series_Folder'\n",
        "start_date = '2020-04-13'\n",
        "end_date = '2021-08-09'\n",
        "data_folder = '/data'\n",
        "separator = '\\t'\n",
        "test_data_percentage = 0.20\n",
        "num_future = 1\n",
        "num_past = 14\n",
        "epochs=10\n",
        "batch_size=15\n",
        "validation_split=0.2\n",
        "\n",
        "def train_neural_network(path = '/content/gdrive/MyDrive/Colab Notebooks/Mutation_As_Time_Series_Folder',\n",
        "                         start_date = '2020-04-13',\n",
        "                         end_date = '2021-08-09',\n",
        "                         data_folder = '/data',\n",
        "                         separator = '\\t',\n",
        "                         test_data_percentage = 0.20,\n",
        "                         num_future = 1,\n",
        "                         num_past = 14,\n",
        "                         epochs=10,\n",
        "                         batch_size=15,\n",
        "                         validation_split=0.2):\n",
        "\n",
        "  trainX, testX, trainY, testY = create_time_series_dataset(start_date,\n",
        "                                                            end_date,\n",
        "                                                            path,\n",
        "                                                            data_folder,\n",
        "                                                            separator,\n",
        "                                                            test_data_percentage,\n",
        "                                                            num_future,\n",
        "                                                            num_past)\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(1000, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences=True))\n",
        "  model.add(Dense(750, activation='relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(500, activation='relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(250, activation='relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(150, activation='relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(50, activation='relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(trainY.shape[1]))\n",
        "\n",
        "  model.compile(optimizer='adam', loss='mse')\n",
        "  model.summary()\n",
        "\n",
        "  history = model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, validation_split=validation_split, verbose=1)\n",
        "  print('Training loss')\n",
        "  print(history.history)\n",
        "\n",
        "  plt.figure(figsize=(15, 6))\n",
        "  plt.plot(history.history['loss'], lw=3, ls='--', label='Loss')\n",
        "  plt.plot(history.history['val_loss'], lw=2, ls='-', label='Val Loss')\n",
        "  plt.xlabel('Epochs', fontsize=15)\n",
        "  plt.ylabel('Loss', fontsize=15)\n",
        "  plt.title('MSE, past: ' + str(num_past) + ', future: ' + str(num_future))\n",
        "  plt.legend()\n",
        "  plt.savefig(path + '/results/past_' + str(num_past) + '_future_' + str(num_future) + '.png')\n",
        "\n",
        "  predicted_output = model.predict(testX, verbose=0)\n",
        "  predicted_mse = mean_squared_error(testY, predicted_output)\n",
        "  print('MSE of Test dataset')\n",
        "  print(predicted_mse)\n",
        "\n",
        "  with open(path + '/results/test_mse_past_' + str(num_past) + '_future_' + str(num_future) + '.txt', 'w') as fp:\n",
        "    fp.write('MSE of Test dataset, past: ' + str(num_past) + ', future: ' + str(num_future) + '\\n')\n",
        "    fp.write(str(predicted_mse))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_hKDWEHNVP4"
      },
      "source": [
        "path = '/content/gdrive/MyDrive/Colab Notebooks/Mutation_As_Time_Series_Folder'\n",
        "start_date = '2020-04-13'\n",
        "end_date = '2021-08-09'\n",
        "data_folder = '/data'\n",
        "separator = '\\t'\n",
        "test_data_percentage = 0.20\n",
        "epochs=50\n",
        "batch_size=15\n",
        "validation_split=0.2\n",
        "\n",
        "for num_past in [7, 14, 21, 28, 35, 42, 49]:\n",
        "  for num_future in [1, 3, 6, 9, 12, 15, 18]:\n",
        "\n",
        "    train_neural_network(path,\n",
        "                         start_date,\n",
        "                         end_date,\n",
        "                         data_folder,\n",
        "                         separator,\n",
        "                         test_data_percentage,\n",
        "                         num_future,\n",
        "                         num_past,\n",
        "                         epochs,\n",
        "                         batch_size,\n",
        "                         validation_split)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bX2IUnTDFFye"
      },
      "source": [
        "#\n",
        "# Graph daily average mutation rate \n",
        "#\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from dateutil import parser\n",
        "\n",
        "df = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/Mutation_As_Time_Series_Folder/data/data_pivoted_2020-04-13_2021-08-09.tsv', sep='\\t')\n",
        "\n",
        "x = df.Collection_Date.values.tolist()\n",
        "x_date = [parser.parse(item) for item in x]\n",
        "\n",
        "print(len(x))\n",
        "\n",
        "y1 = df['0']\n",
        "y2 = df['1']\n",
        "y3 = df['2']\n",
        "y4 = df['3']\n",
        "y5 = df['4']\n",
        "y6 = df['5']\n",
        "y7 = df['6']\n",
        "y8 = df['7']\n",
        "y9 = df['8']\n",
        "y10 = df['9']\n",
        "y11 = df['10']\n",
        "y12 = df['11']\n",
        "\n",
        "fig, ax = plt.subplots(6, 2)\n",
        "\n",
        "fig.set_size_inches(20.0, 20.0)\n",
        "fig.subplots_adjust(hspace=1.5)\n",
        "\n",
        "plt.subplot(6, 2, 1)\n",
        "plt.plot(x, y1, 'r', label='A -> T')\n",
        "plt.legend()\n",
        "plt.xticks(x[::20], rotation=25)\n",
        "\n",
        "plt.subplot(6, 2, 2)\n",
        "plt.plot(x, y2, 'b', label='A -> G')\n",
        "plt.legend()\n",
        "plt.xticks(x[::20], rotation=25)\n",
        "\n",
        "plt.subplot(6, 2, 3)\n",
        "plt.plot(x, y3, 'g', label='A -> C')\n",
        "plt.legend()\n",
        "plt.xticks(x[::20], rotation=25)\n",
        "\n",
        "plt.subplot(6, 2, 4)\n",
        "plt.plot(x, y4, 'r', label='T -> G')\n",
        "plt.legend()\n",
        "plt.xticks(x[::20], rotation=25)\n",
        "\n",
        "plt.subplot(6, 2, 5)\n",
        "plt.plot(x, y5, 'b', label='T -> C')\n",
        "plt.legend()\n",
        "plt.xticks(x[::20], rotation=25)\n",
        "\n",
        "plt.subplot(6, 2, 6)\n",
        "plt.plot(x, y6, 'g', label='T -> A')\n",
        "plt.legend()\n",
        "plt.xticks(x[::20], rotation=25)\n",
        "\n",
        "plt.subplot(6, 2, 7)\n",
        "plt.plot(x, y7, 'r', label='G -> C')\n",
        "plt.legend()\n",
        "plt.xticks(x[::20], rotation=25)\n",
        "\n",
        "plt.subplot(6, 2, 8)\n",
        "plt.plot(x, y8, 'b', label='G -> A')\n",
        "plt.legend()\n",
        "plt.xticks(x[::20], rotation=25)\n",
        "\n",
        "plt.subplot(6, 2, 9)\n",
        "plt.plot(x, y9, 'g', label='G -> T')\n",
        "plt.legend()\n",
        "plt.xticks(x[::20], rotation=25)\n",
        "\n",
        "plt.subplot(6, 2, 10)\n",
        "plt.plot(x, y10, 'r', label='C -> A')\n",
        "plt.legend()\n",
        "plt.xticks(x[::20], rotation=25)\n",
        "\n",
        "plt.subplot(6, 2, 11)\n",
        "plt.plot(x, y11, 'b', label='C -> T')\n",
        "plt.legend()\n",
        "plt.xticks(x[::20], rotation=25)\n",
        "\n",
        "plt.subplot(6, 2, 12)\n",
        "plt.plot(x, y12, 'g', label='C -> G')\n",
        "plt.legend()\n",
        "plt.xticks(x[::20], rotation=25)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yS1IKZTbN4F5"
      },
      "source": [
        "'''\n",
        "# Make a single prediction\n",
        "testX_0 = testX[0].reshape(1, testX.shape[1], testX.shape[2])\n",
        "testY_0 = testY[0].reshape(1, testY.shape[1])\n",
        "\n",
        "print('Predicted output')\n",
        "predicted_output_0 = model.predict(testX_0, verbose=0)\n",
        "print(predicted_output_0)\n",
        "print('Actual output')\n",
        "print(testY_0)\n",
        "print('MSE of prediction')\n",
        "np.sqrt(mean_squared_error(testY_0, predicted_output_0))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}